// span-wip -db amsl.db < in > out
//
// For each record:
//
// [ ] select rows from db (sid, mc, tcid)
// [ ] if there is a holdingfile, and not cached, download, cache, parse and cache it
// [ ] match against holdingfile
// [ ] collect all ISIL and attach them (or just print id and isil)
// [ ] parallelize by copying the database (sqlite3 is single threaded), e.g. 4x NumCPU of the like
//
// Also, try to get rid of span-freeze by using some locally cached files (with
// expiry date).
package main

import (
	"archive/zip"
	"bufio"
	"crypto/sha1"
	"encoding/json"
	"flag"
	"fmt"
	"io"
	"io/ioutil"
	"os"
	"path"
	"path/filepath"
	"sort"
	"strings"
	"time"

	"github.com/adrg/xdg"
	_ "github.com/mattn/go-sqlite3"
	"github.com/sethgrid/pester"

	"github.com/jmoiron/sqlx"
	"github.com/miku/span/atomic"
	"github.com/miku/span/formats/finc"
	"github.com/miku/span/licensing"
	"github.com/miku/span/licensing/kbart"

	log "github.com/sirupsen/logrus"
)

var (
	force  = flag.Bool("f", false, "force all external referenced links to be downloaded")
	dbFile = flag.String("db", "", "path to an sqlite3 file generated by span-amsl-discovery -db file.db ...")
)

// ConfigRow decribing a single entry (e.g. an attachment request).
type ConfigRow struct {
	ShardLabel                     string
	ISIL                           string
	SourceID                       string
	TechnicalCollectionID          string
	MegaCollection                 string
	HoldingsFileURI                string
	HoldingsFileLabel              string
	LinkToHoldingsFile             string
	EvaluateHoldingsFileForLibrary string
	ContentFileURI                 string
	ContentFileLabel               string
	LinkToContentFile              string
	ExternalLinkToContentFile      string
	ProductISIL                    string
	DokumentURI                    string
	DokumentLabel                  string
}

// HFCache wraps access to holdings files.
type HFCache struct {
	// entries maps a link or filename (or any identifier) to a map from ISSN
	// to licensing entries.
	entries map[string]map[string][]licensing.Entry
}

// cacheFilename returns the path to the locally cached version of this URL.
func (c *HFCache) cacheFilename(hflink string) string {
	h := sha1.New()
	_, _ = io.WriteString(h, hflink)
	return filepath.Join(xdg.CacheHome, "span", fmt.Sprintf("%x", h.Sum(nil)))
}

// populate fills the entries map from a given URL.
func (c *HFCache) populate(hflink string) error {
	if _, ok := c.entries[hflink]; ok {
		return nil
	}
	filename := c.cacheFilename(hflink)
	if fi, err := os.Stat(path.Dir(filename)); os.IsNotExist(err) {
		if err := os.MkdirAll(path.Dir(filename), 0755); err != nil {
			return err
		}
	} else if !fi.IsDir() {
		return fmt.Errorf("expected cache directory at: %s", path.Dir(filename))
	}
	if _, err := os.Stat(filename); os.IsNotExist(err) {
		resp, err := pester.Get(hflink)
		if err != nil {
			return err
		}
		defer resp.Body.Close()
		b, err := ioutil.ReadAll(resp.Body)
		if err != nil {
			return err
		}
		if err := atomic.WriteFile(filename, b, 0644); err != nil {
			return err
		}
	}
	h := new(kbart.Holdings)
	zr, err := zip.OpenReader(filename)
	if err == nil {
		defer zr.Close()
		for _, f := range zr.File {
			rc, err := f.Open()
			if err != nil {
				return err
			}
			if _, err := h.ReadFrom(rc); err != nil {
				return err
			}
			rc.Close()
		}
	} else {
		f, err := os.Open(filename)
		if err != nil {
			return err
		}
		defer f.Close()
		if _, err := h.ReadFrom(f); err != nil {
			return err
		}
	}
	snm := h.SerialNumberMap()
	c.entries[hflink] = h.SerialNumberMap()
	if len(snm) == 0 {
		log.Printf("warning: %s is maybe not a KBART file", hflink)
	} else {
		log.Printf("parsed %s", hflink)
		log.Printf("parsed %d entries from %s (%d)",
			len(c.entries[hflink]), filename, len(c.entries))
	}
	return nil
}

// Covers returns true, if a holdings file, given by link or filename, covers
// the document. The cache takes care of downloading the file, if necessary.
func (c *HFCache) Covers(hflink string, doc *finc.IntermediateSchema) (ok bool, err error) {
	if err = c.populate(hflink); err != nil {
		return false, err
	}
	for _, issn := range append(doc.ISSN, doc.EISSN...) {
		for _, entry := range c.entries[hflink][issn] {
			err = entry.Covers(doc.RawDate, doc.Volume, doc.Issue)
			if err == nil {
				return true, nil
			}
		}
		// TODO(miku): gather debug information
	}
	return false, nil
}

// cacheKey returns a key for a document, containing a subset (e.g. sid and
// collcetions) of fields, e.g.  to be used to cache subset of the about 250k
// rows currently in AMSL.
func cacheKey(doc *finc.IntermediateSchema) string {
	v := doc.MegaCollections
	sort.Strings(v)
	return doc.SourceID + "@" + strings.Join(v, "@")
}

// Labeler updates an intermediate schema document.
// We need mostly: ISIL, SourceID, MegaCollection, TechnicalCollectionID, HoldFileURI,
// EvaluateHoldingsFileForLibrary
type Labeler struct {
	dbFile string
	db     *sqlx.DB

	cache   map[string][]ConfigRow
	hfcache *HFCache
}

// open opens the database connection, read-only.
func (l *Labeler) open() (err error) {
	if l.db != nil {
		return nil
	}
	dsn := fmt.Sprintf("%s?ro=1", l.dbFile)
	l.db, err = sqlx.Connect("sqlite3", dsn)
	if err != nil {
		return err
	}
	return nil
}

// matchingRows returns a list of relevant rows for a given document.
func (l *Labeler) matchingRows(doc *finc.IntermediateSchema) (result []ConfigRow, err error) {
	if l.cache == nil {
		l.cache = make(map[string][]ConfigRow)
	}
	key := cacheKey(doc)
	if v, ok := l.cache[key]; ok {
		return v, nil
	}
	if len(doc.MegaCollections) == 0 {
		return result, nil
	}
	// At a minimum, the sid and tcid or collection name must match.
	q, args, err := sqlx.In(`
		SELECT isil, sid, tcid, mc, hflink, hfeval, cflink, cfelink FROM amsl WHERE sid = ? AND (mc IN (?) OR tcid IN (?))
	`, doc.SourceID, doc.MegaCollections, doc.MegaCollections)
	if err != nil {
		return nil, err
	}
	rows, err := l.db.Query(q, args...)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	for rows.Next() {
		var cr ConfigRow
		err = rows.Scan(&cr.ISIL,
			&cr.SourceID,
			&cr.TechnicalCollectionID,
			&cr.MegaCollection,
			&cr.LinkToHoldingsFile,
			&cr.EvaluateHoldingsFileForLibrary,
			&cr.ContentFileURI,
			&cr.ExternalLinkToContentFile)
		if err != nil {
			return nil, err
		}
		result = append(result, cr)
	}
	l.cache[key] = result
	return result, nil
}

// Label updates document in place.
func (l *Labeler) Label(doc *finc.IntermediateSchema) error {
	if err := l.open(); err != nil {
		return err
	}
	rows, err := l.matchingRows(doc)
	if err != nil {
		return err
	}
	var labels = make(map[string]struct{}) // ISIL to attach

	// Distinguish cases, e.g. with or w/o HF, https://git.io/JvdmC.
	for _, row := range rows {
		switch {
		case row.EvaluateHoldingsFileForLibrary == "no" && row.LinkToHoldingsFile != "":
			return fmt.Errorf("config provides holding file, but does not want to evaluate it: %v", row)
		case row.EvaluateHoldingsFileForLibrary == "yes" && row.LinkToHoldingsFile != "":
			ok, err := l.hfcache.Covers(row.LinkToHoldingsFile, doc)
			if err != nil {
				return err
			}
			if ok {
				labels[row.ISIL] = struct{}{}
			}
		case row.EvaluateHoldingsFileForLibrary == "no":
			labels[row.ISIL] = struct{}{}
		case row.ContentFileURI != "":
		default:
			return fmt.Errorf("none of the attachment modes match for %v", doc)
		}
	}

	var keys []string
	for k := range labels {
		keys = append(keys, k)
	}
	fmt.Printf("%s\t%s\n", doc.ID, strings.Join(keys, ", "))
	return nil
}

func main() {
	flag.Parse()
	if *dbFile == "" {
		log.Fatal("we need a configuration database")
	}
	var (
		hfcache = &HFCache{entries: make(map[string]map[string][]licensing.Entry)}
		labeler = &Labeler{dbFile: *dbFile, hfcache: hfcache}
		br      = bufio.NewReader(os.Stdin)
		i       = 0
		started = time.Now()
	)
	for {
		if i%10000 == 0 {
			log.Printf("%d %0.2f", i, float64(i)/time.Since(started).Seconds())
		}
		b, err := br.ReadBytes('\n')
		if err == io.EOF {
			break
		}
		if err != nil {
			log.Fatal(err)
		}
		var doc finc.IntermediateSchema // TODO: try reduced schema
		if err := json.Unmarshal(b, &doc); err != nil {
			log.Fatal(err)
		}
		if err := labeler.Label(&doc); err != nil {
			log.Fatal(err)
		}
		i++
	}
}
